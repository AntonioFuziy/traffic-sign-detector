{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44c2524",
   "metadata": {},
   "source": [
    "# Running\n",
    "\n",
    "To run the Traffic Sign's detector model, one of the ways of doing that is using Nvidia SDK: DeepStream [[Official documentation here](https://developer.nvidia.com/deepstream-sdk)], so follow the steps below:\n",
    "### 1. Getting the Docker Container\n",
    "**Attention:** To make use of the detector with your own Webcam, you must disable all host running this command:\n",
    "```console\n",
    "xhost +\n",
    "```\n",
    "We will use a Docker Container to get in touch with the DeepStream Toolkit. For this project (specifically), a modified version of the Docker image was posted [[here](https://hub.docker.com/r/edgardaon/nvidia-deepstream-6.0-triton)] aiming to spare the user from the need of software configuration and installation. Run the command down below to obtain a *bash* with all the project infrastructure:  \n",
    "```console\n",
    "docker run --gpus all -it --rm --net=host --privileged -v /tmp/.X11-unix:/tmp/.X11-unix -w /opt/nvidia/deepstream/deepstream-6.0 edgardaon/nvidia-deepstream-6.0-triton:projetoML\n",
    "```\n",
    "\n",
    "**Attention:** However, if you don't want this version but do want to do a fresh configuration of the SDK all by yourself, feel free to follow the steps from the Nvidia NGC: [https://catalog.ngc.nvidia.com/orgs/nvidia/containers/deepstream](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/deepstream) After that, you should clone the [NVIDIA-IOT's DeepStream Python Apps Repository](https://github.com/NVIDIA-AI-IOT/deepstream_python_apps) to make use of python along with DeepStream.\n",
    "\n",
    "### 2. Inside the Docker Container\n",
    "Inside the container, run the command: ***deepstream-app --version-all***. If everything is set, you should see this versions in the output below:\n",
    "\n",
    "\n",
    "Software  | Version\n",
    "------------- | -------------\n",
    "deepstream-app  | 6.0.0\n",
    "DeepStreamSDK  | 6.0.0 \n",
    "CUDA Driver  | 11.7\n",
    "CUDA Runtime  | 11.4\n",
    "TensorRT   | 8.0\n",
    "cuDNN   | 8.2\n",
    "libNVWarp360   | 2.0.1d3\n",
    "\n",
    "Then, go to the directory where the project files are located, which is ***/opt/nvidia/deepstream/deepstream-6.0/sources/deepstream_python_apps/apps/deepstream-projeto/***. There you shold see the python script (*deepstream_test_1_usb.py*), the DeepStream configuration file (*dstest1_pgie_config.txt*), the model labels (*labels.txt*), the detector encoded model (*resnet18_detector.etlt*) (*.etlt* extension stands for encoded transfer learning toolkit, a file created by the TAO Toolkit), the detector engine (*resnet18_int8_detector.engine*) and the last, but not least, calibration file (*calibration.bin*).\n",
    "\n",
    "\n",
    "Before running the detector, it is important to check Deepstream configuration file:\n",
    "```text\n",
    "[property]\n",
    "gpu-id=0\n",
    "net-scale-factor=0.0039215697906911373\n",
    "model-engine-file=./resnet18_int8_detector.engine\n",
    "int8-calib-file=./calibration.bin\n",
    "labelfile-path=./labels.txt\n",
    "tlt-encoded-model=./resnet18_detector.etlt\n",
    "tlt-model-key=roboflow\n",
    "infer-dims=3;640;640\n",
    "uff-input-order=0\n",
    "uff-input-blob-name=input_1\n",
    "batch-size=1\n",
    "process-mode=1\n",
    "model-color-format=0\n",
    "## 0=FP32, 1=INT8, 2=FP16 mode\n",
    "network-mode=1\n",
    "num-detected-classes=3\n",
    "interval=0\n",
    "gie-unique-id=1\n",
    "output-blob-names=output_bbox/BiasAdd;output_cov/Sigmoid\n",
    "\n",
    "[class-attrs-all]\n",
    "pre-cluster-threshold=0.2\n",
    "eps=0.2\n",
    "group-threshold=1\n",
    "\n",
    "```\n",
    "**Note**: If you haven't used the *edgardaon/nvidia-deepstream-6.0-triton* docker image (referenced in the docker run command above) and is running another one, like the default from *NGC (Nvidia GPU Cloud) Catalog*, you should upload the *deepstream_test_1_usb.py* project python script to a any folder as long as it has the others TAO exported files also.\n",
    "\n",
    "There are some properties that are worth noting, like *gpu_id* ensure us that the model will run at the main GPU, the *model-engine-file* needs to be the exact detector *.engine* file and the same goes for the *int8-calib-file*, *labelfile-path*, *tlt-encoded-model* and *tlt-model-key*, receiving the *calibration.bin*, *labels.txt*, *resnet18_detector.etlt* files with *roboflow* password. After that, there's the *num-detected-classes* which are going to be 3 since this is the number types of signs we have to detect: *stop* (\"pare\"), *40* and *60*.\n",
    "After configuring these, we can run the detector using the command below, where */dev/video0* is the webcam address for this example:\n",
    "```console\n",
    "python3 deepstream_test_1_usb.py /dev/video0\n",
    "```\n",
    "Running this code, you should see in the upper left corner a frame, a sign total, stop sign, 40 sign and 60 sign counters with the detections (if they may occur) on screen. Below, we have a demonstration of the model detecting stop signs, 40 signs and 60 signs, respectively.\n",
    "\n",
    "\n",
    "#### Demonstrations:\n",
    "<div align=\"center\">\n",
    "    <img src=\"../Videos/placaPARE.gif\" />\n",
    "    <img src=\"../Videos/placa40.gif\" />\n",
    "    <img src=\"../Videos/placa60.gif\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172df31d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce7db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
